{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hashlib import md5\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fake labelled samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base_dir = '/home/easton/projects/tfhouse_v1/egs/aishell2/exp/AISHELL-2/transformer_saa_encoder_4gpu_320_2560_ctc_ce_loss/'\n",
    "# output_target = base_dir + 'targets_of_beam16_from_average8ckpts-0'\n",
    "# output_predict = base_dir + 'preds_of_beam16_from_average8ckpts-0'\n",
    "\n",
    "# base_dir = '/data3/easton/data/AISHELL-2/random_100h/org_data/random-100_plus_100-of-900/'\n",
    "# output_target = base_dir + 'targets_of_beam16_from_average8ckpts-0'\n",
    "# output_predict = base_dir + 'preds_of_beam16_from_average8ckpts-0'\n",
    "\n",
    "base_dir = '/home/easton/projects/tfhouse/egs_v3/aishell-2/exp/transformer_random100h/'\n",
    "output_target = base_dir + '16_beam100-of-900_from_average8ckpts-0.top4.ref'\n",
    "output_predict = base_dir + '16_beam100-of-900_from_average8ckpts-0.top4.res'\n",
    "dict_md52trans = {}\n",
    "with open(output_target) as f1, open(output_predict) as f2:\n",
    "    for i, (l1, l2) in enumerate(zip(f1, f2)):\n",
    "        if i % 4 != 0: continue\n",
    "        \n",
    "        try:\n",
    "            target = l1.strip().split()[1]\n",
    "            key = md5(target.encode('utf-8')).hexdigest()\n",
    "            predits = l2.strip().split()[1]\n",
    "            dict_md52trans[key] = predits\n",
    "        except:\n",
    "            print(l1, l2)\n",
    "            continue\n",
    "\n",
    "real_labelled = '/data3/easton/data/AISHELL-2/random_100h/org_data/100-of-900/trans.char'\n",
    "fake_labelled ='/data3/easton/data/AISHELL-2/random_100h/org_data/100+100-of-900/trans-pred_100h.char'\n",
    "with open(real_labelled) as f, open(fake_labelled, 'w') as fw:\n",
    "    for line in f:\n",
    "        try:\n",
    "            uttid, trans = line.strip().split(maxsplit=1)\n",
    "            key = md5(trans.replace(' ', '').encode('utf-8')).hexdigest()\n",
    "\n",
    "            new_line = uttid + ' ' + ' '.join(dict_md52trans[key])\n",
    "            fw.write(new_line + '\\n')\n",
    "        except KeyError:\n",
    "            print(uttid, trans.replace(' ', ''))\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fixed2trans\n",
    "把修正后的识别结果转化为trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_trans = '/data3/easton/data/AISHELL-2/random_100h/org_data/100-of-900/trans.char'\n",
    "f_fix = '/home/easton/projects/LM_EVAL/preds_top4.probs.fixed'\n",
    "f_fixed_trans = '/home/easton/projects/LM_EVAL/100-of-900_lm-fixed.trans'\n",
    "f_org_trans = '/home/easton/projects/LM_EVAL/100-of-900_asr-output.trans'\n",
    "\n",
    "dict_trans2uttid = {}\n",
    "with open(f_trans) as f:\n",
    "    for line in f:\n",
    "        uttid, trans = line.strip().split(' ', 1)\n",
    "        key = md5(trans.replace(' ', '').encode('utf-8')).hexdigest()\n",
    "        dict_trans2uttid[key] = uttid\n",
    "\n",
    "i = True\n",
    "with open(f_fix) as f, open(f_org_trans, 'w') as fw1, open(f_fixed_trans, 'w') as fw2:\n",
    "    for line in f:\n",
    "        res = line.strip().split(',', 2)\n",
    "        key = md5(res[0].split(':')[1].encode('utf-8')).hexdigest()\n",
    "        uttid = dict_trans2uttid[key]\n",
    "        trans_org = res[1].split(':')[1]\n",
    "        try:\n",
    "            trans_to_fix = res[2].split(':', 1)[1]\n",
    "            if i: print(trans_to_fix); i = False\n",
    "            list_trans = []\n",
    "            for group in re.findall('([^()]+)', trans_to_fix):\n",
    "                if ':' in group:\n",
    "                    token = group.split(',')[0].split(':')[0]\n",
    "                    list_trans.append(token)\n",
    "                else:\n",
    "                    list_trans.append(group)\n",
    "            trans_fix = ''.join(list_trans)\n",
    "        except:\n",
    "            trans_fix = None\n",
    "            \n",
    "        fw1.write(uttid + ' ' + ' '.join(trans_org) + '\\n')\n",
    "        if trans_fix:\n",
    "            fw2.write(uttid + ' ' + ' '.join(trans_fix) + '\\n')\n",
    "        else:\n",
    "            fw2.write(uttid + ' ' + ' '.join(trans_org) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'给我把空调温度切换到二十一'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data3/easton/data/AISHELL-2/random_100h/org_data/100-of-900/trans.char'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7ab4c03bb2e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdict_trans2uttid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_trans\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0muttid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data3/easton/data/AISHELL-2/random_100h/org_data/100-of-900/trans.char'"
     ]
    }
   ],
   "source": [
    "f_trans = '/data3/easton/data/AISHELL-2/random_100h/org_data/100-of-900/trans.char'\n",
    "\n",
    "base_dir = '/home/easton/projects/tfhouse/egs_v3/aishell-2/exp/transformer_random100h/'\n",
    "output_target = base_dir + '16_beam100-of-900_from_average8ckpts-0.top4.ref'\n",
    "output_predict = base_dir + '16_beam100-of-900_from_average8ckpts-0.top4.res'\n",
    "\n",
    "dict_trans2uttid = {}\n",
    "with open(f_trans) as f:\n",
    "    for line in f:\n",
    "        uttid, trans = line.strip().split(' ', 1)\n",
    "        key = md5(trans.replace(' ', '').encode('utf-8')).hexdigest()\n",
    "        dict_trans2uttid[key] = uttid\n",
    "\n",
    "i = True\n",
    "with open(f_fix) as f, open(f_org_trans, 'w') as fw1, open(f_fixed_trans, 'w') as fw2:\n",
    "    for line in f:\n",
    "        res = line.strip().split(',', 2)\n",
    "        key = md5(res[0].split(':')[1].encode('utf-8')).hexdigest()\n",
    "        uttid = dict_trans2uttid[key]\n",
    "        trans_org = res[1].split(':')[1]\n",
    "        try:\n",
    "            trans_to_fix = res[2].split(':', 1)[1]\n",
    "            if i: print(trans_to_fix); i = False\n",
    "            list_trans = []\n",
    "            for group in re.findall('([^()]+)', trans_to_fix):\n",
    "                if ':' in group:\n",
    "                    token = group.split(',')[0].split(':')[0]\n",
    "                    list_trans.append(token)\n",
    "                else:\n",
    "                    list_trans.append(group)\n",
    "            trans_fix = ''.join(list_trans)\n",
    "        except:\n",
    "            trans_fix = None\n",
    "            \n",
    "        fw1.write(uttid + ' ' + ' '.join(trans_org) + '\\n')\n",
    "        if trans_fix:\n",
    "            fw2.write(uttid + ' ' + ' '.join(trans_fix) + '\\n')\n",
    "        else:\n",
    "            fw2.write(uttid + ' ' + ' '.join(trans_org) + '\\n')\n",
    "\n",
    "dict_trans2uttid = {}\n",
    "with open(f_trans) as f:\n",
    "    for line in f:\n",
    "        uttid, trans = line.strip().split(' ', 1)\n",
    "        key = md5(trans.replace(' ', '').encode('utf-8')).hexdigest()\n",
    "        dict_trans2uttid[key] = uttid\n",
    "\n",
    "i = True\n",
    "with open(f_fix) as f, open(f_org_trans, 'w') as fw1, open(f_fixed_trans, 'w') as fw2:\n",
    "    for line in f:\n",
    "        res = line.strip().split(',', 2)\n",
    "        key = md5(res[0].split(':')[1].encode('utf-8')).hexdigest()\n",
    "        uttid = dict_trans2uttid[key]\n",
    "        trans_org = res[1].split(':')[1]\n",
    "        try:\n",
    "            trans_to_fix = res[2].split(':', 1)[1]\n",
    "            if i: print(trans_to_fix); i = False\n",
    "            list_trans = []\n",
    "            for group in re.findall('([^()]+)', trans_to_fix):\n",
    "                if ':' in group:\n",
    "                    token = group.split(',')[0].split(':')[0]\n",
    "                    list_trans.append(token)\n",
    "                else:\n",
    "                    list_trans.append(group)\n",
    "            trans_fix = ''.join(list_trans)\n",
    "        except:\n",
    "            trans_fix = None\n",
    "            \n",
    "        fw1.write(uttid + ' ' + ' '.join(trans_org) + '\\n')\n",
    "        if trans_fix:\n",
    "            fw2.write(uttid + ' ' + ' '.join(trans_fix) + '\\n')\n",
    "        else:\n",
    "            fw2.write(uttid + ' ' + ' '.join(trans_org) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utt1 ['utt1', 'utt1', 'utt1', 'utt1']\n",
      "utt2 ['utt2', 'utt2', 'utt2']\n",
      "utt3 ['utt3', 'utt3', 'utt3', 'utt3']\n",
      "utt4 []\n",
      "utt5 ['utt5', 'utt5', 'utt5', 'utt5']\n",
      "utt6 ['utt6', 'utt6', 'utt6', 'utt6']\n",
      "utt7 ['utt7', 'utt7', 'utt7', 'utt7']\n",
      "utt8 ['utt8', 'utt8', 'utt8', 'utt8']\n",
      "utt9 []\n",
      "utt10 []\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1493d204f927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0midx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f_trans = '/data3/easton/data/AISHELL-2/random_100h/org_data/100-of-900/trans.char'\n",
    "\n",
    "dict_trans2uttid = {}\n",
    "with open(f_trans) as f:\n",
    "    for line in f:\n",
    "        uttid, trans = line.strip().split(' ', 1)\n",
    "        key = md5(trans.replace(' ', '').encode('utf-8')).hexdigest()\n",
    "        dict_trans2uttid[key] = uttid\n",
    "        \n",
    "f1 = open('beam16_decoded4.res')\n",
    "f2 = open('beam16_decoded4.ref')\n",
    "\n",
    "list_beam = []\n",
    "\n",
    "with open('beam16_decoded.trans', 'w') as fw:\n",
    "    while True:\n",
    "        for _ in range(4):\n",
    "            l2 = next(f2)\n",
    "            idx2, ref = l2.strip().split(' ', 1)\n",
    "\n",
    "        key = md5(ref.encode('utf-8')).hexdigest()\n",
    "        uttid = dict_trans2uttid[ref]\n",
    "        idx = list_beam[-1][0] if list_beam else idx2\n",
    "        while idx == idx2:\n",
    "            l1 = next(f1)\n",
    "            idx, preds, _, ppl = l1.strip().split(',', 3)\n",
    "            idx = idx.split(':')[1]\n",
    "            preds = preds.split(':')[1]\n",
    "            ppl = float(ppl.split(':')[1])\n",
    "            list_beam.append((idx, preds, ppl))\n",
    "        tmp = list_beam[-1]\n",
    "        list_beam = list_beam[: -1]\n",
    "        assert len(list_beam) <= 4\n",
    "\n",
    "        if list_beam:\n",
    "            list_beam.sort(key=lambda x: x[-1])\n",
    "            preds = list_beam[-1][1]\n",
    "            line = uttid + ' ' + preds\n",
    "            fw.write(line + '\\n')\n",
    "        \n",
    "        list_beam = [tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id:utt1,preds:宜 佳 家 居 北 京 商 场 店,score_lm:0.003 0.001 0.958 0.672 0.326 0.448 0.065 0.034 0.000,ppl:0.28\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
