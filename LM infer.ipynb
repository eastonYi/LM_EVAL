{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import modeling\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_builder(bert_config, init_checkpoint):\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "    input_ids = tf.placeholder(tf.int32, [None, None], name='input_ids')\n",
    "    input_mask = tf.placeholder(tf.int32, [None, None], name='input_mask')\n",
    "    masked_lm_positions = tf.placeholder(tf.int32, [None, None], name='masked_lm_positions')\n",
    "    segment_ids = tf.zeros_like(input_mask)\n",
    "\n",
    "    model = modeling.BertModel(\n",
    "            config=bert_config,\n",
    "            is_training=False,\n",
    "            input_ids=input_ids,\n",
    "            input_mask=(input_mask),\n",
    "            token_type_ids=segment_ids)\n",
    "\n",
    "    log_probs = get_masked_lm_output(\n",
    "            bert_config,\n",
    "            model.get_sequence_output(),\n",
    "            model.get_embedding_table(),\n",
    "            masked_lm_positions)\n",
    "\n",
    "    outputs = tf.reshape(\n",
    "        log_probs, [tf.shape(masked_lm_positions)[0], tf.shape(masked_lm_positions)[1], bert_config.vocab_size])\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    initialized_variable_names = {}\n",
    "\n",
    "    if init_checkpoint:\n",
    "        (assignment_map, initialized_variable_names\n",
    "        ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "    inputs = (input_ids, input_mask, masked_lm_positions, masked_lm_positions)\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "def get_masked_lm_output(bert_config, input_tensor, output_weights, positions):\n",
    "    \"\"\"Get loss and log probs for the masked LM.\"\"\"\n",
    "    input_tensor = gather_indexes(input_tensor, positions)\n",
    "\n",
    "    with tf.variable_scope(\"cls/predictions\"):\n",
    "        # We apply one more non-linear transformation before the output layer.\n",
    "        # This matrix is not used after pre-training.\n",
    "        with tf.variable_scope(\"transform\"):\n",
    "            input_tensor = tf.layers.dense(\n",
    "                    input_tensor,\n",
    "                    units=bert_config.hidden_size,\n",
    "                    activation=modeling.get_activation(bert_config.hidden_act),\n",
    "                    kernel_initializer=modeling.create_initializer(\n",
    "                            bert_config.initializer_range))\n",
    "            input_tensor = modeling.layer_norm(input_tensor)\n",
    "\n",
    "        output_bias = tf.get_variable(\n",
    "                \"output_bias\",\n",
    "                shape=[bert_config.vocab_size],\n",
    "                initializer=tf.zeros_initializer())\n",
    "        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    return log_probs\n",
    "\n",
    "\n",
    "def gather_indexes(sequence_tensor, positions):\n",
    "    \"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"\n",
    "    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n",
    "    batch_size = sequence_shape[0]\n",
    "    seq_length = sequence_shape[1]\n",
    "    width = sequence_shape[2]\n",
    "\n",
    "    flat_offsets = tf.reshape(\n",
    "        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
    "    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
    "    flat_sequence_tensor = tf.reshape(sequence_tensor,\n",
    "                                      [batch_size * seq_length, width])\n",
    "    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Running Fixing *****\n",
      "WARNING:tensorflow:From /Users/easton/Public/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/easton/Projects/LM_EVAL/modeling.py:673: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "bert_path = '/Users/easton/Data/pre-train/chinese_L-12_H-768_A-12/'\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=bert_path +'vocab.txt', do_lower_case=True)\n",
    "MASKED_TOKEN = \"[MASK]\"\n",
    "MASKED_ID = tokenizer.convert_tokens_to_ids([MASKED_TOKEN])[0]\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "tf.logging.info(\"***** Running Fixing *****\")\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(bert_path + 'bert_config.json')\n",
    "input_pl, log_prob_op = model_builder(bert_config, bert_path + 'bert_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/easton/Public/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_masked_lm_prediction(input_ids, mask_position):\n",
    "    new_input_ids = list(input_ids)\n",
    "    masked_lm_positions = list(range(mask_position, mask_position+1))\n",
    "    for i in masked_lm_positions:\n",
    "        new_input_ids[i] = MASKED_ID\n",
    "\n",
    "    return new_input_ids, masked_lm_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1762, 704, 1744, 5307, 3845, 3173, 103, 1378, 4638, 1920, 5520, 3250, 678, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 30\n",
    "tokens = list('在中国经济新强台的大背景下')\n",
    "# tokens = list('床前白月光,疑是地上霜')\n",
    "index = 6\n",
    "masked_lm_positions = []\n",
    "\n",
    "input_tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "len_pad = max_seq_length - len(input_tokens)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(input_tokens) + [0] * len_pad\n",
    "input_mask = [1] * len(input_tokens) + [0] * len_pad\n",
    "new_input_ids, masked_lm_positions = create_masked_lm_prediction(input_ids, index+1)\n",
    "\n",
    "pad_len = max_seq_length - len(masked_lm_positions)\n",
    "masked_lm_positions += [0] * pad_len\n",
    "\n",
    "print(new_input_ids, input_mask, masked_lm_positions)\n",
    "dict_feed = {input_pl[0]: [new_input_ids],\n",
    "             input_pl[1]: [input_mask],\n",
    "             input_pl[2]: [masked_lm_positions]}\n",
    "log_probs = sess.run(log_prob_op, feed_dict=dict_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.2652958e-06, 1.8304976e-05]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.exp(log_probs[0][0][i]) for i in tokenizer.convert_tokens_to_ids(['强', '常'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3209"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.exp(log_probs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_tf1",
   "language": "python",
   "name": "py36_tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
